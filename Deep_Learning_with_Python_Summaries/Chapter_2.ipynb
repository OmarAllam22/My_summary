{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e7b158",
   "metadata": {},
   "source": [
    " # Chapter_2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492f049",
   "metadata": {},
   "source": [
    "## Tensors:\n",
    ">>* **Scalars** are **`Rank-0 Tensor`** ... `np.array(22)` ... `np.array(22).ndim`\n",
    ">>* **vectors** are **`Rank-1 Tensor`** ... `np.array([22])` ... `np.array([22]).ndim`\n",
    ">>* **Matrices** can be **`Rank-n Tensor`** ... `np.array([[22]])` ... `np.array([[22]]).ndim`\n",
    ">* usually, we deal with `Rank 0 -> 4` but we can deal with `Rank-5 tensors` if we process **video data**\n",
    "\n",
    "### Real-world examples of Tensors:\n",
    ">* **Vector data**—`Rank-2` tensors of shape `(samples, features)` samples are number of instances.\n",
    ">>* A dataset of text documents, where we represent each document by the counts of how many times each word appears in it (out of a dictionary of 20,000 common words).`shape (500, 20000)`\n",
    "\n",
    ">* **Timeseries data or sequence data**—`Rank-3` tensors of shape `(samples, timesteps, features)` audio data falls into this category also.\n",
    ">>* A dataset of stock prices. **Every minute**, we store the `current price` of the stock, the `highest price in the past minute`, and the `lowest price in the past minute`. Thus, every minute is encoded as a 3D vector, an entire day of trading is encoded as a matrix of shape (390, 3) (there are 390 minutes in a trading day), and 250 days’ worth of data can be stored in a rank-3 tensor of `shape (250, 390, 3)`. Here, **each instance would be one day’s worth of data.**\n",
    "\n",
    ">* **Images**—`Rank-4` tensors of shape `(samples, height, width, channels)`, where each sample is a 2D grid of pixels, and each pixel is represented by a vector of values (“channels”)\n",
    ">>* A batch of 128 **grayscale images** of size 256 × 256 could thus be stored in a tensor of `shape (128, 256, 256, 1)`\n",
    ">>* A batch of 128 **RGB images** or **HSV images** could be stored in a tensor of `shape (128, 256, 256, 3)`\n",
    ">>> Note: there are two formats of images: `(samples, channels, height, width)`, `(samples, height, width, channels)`. The Keras API provides support for both formats. but the second is the Tensorflow standard one.\n",
    "\n",
    ">* **Video**—`Rank-5` tensors of shape `(samples, frames, height, width, channels)`, where each sample is a sequence (of length frames) of images.\n",
    ">>* A 60-second, 144 × 256 YouTube video clip sampled at 4 frames per second would have 240 frames. A batch of 7 such video clips would be stored in a tensor of `shape (7, 240, 144, 256, 3)`. That’s a total of **106,168,320** values! If the dtype of the tensor was `float32`, each value would be stored in `32 bits`, so the tensor would represent `405 MB`. Heavy! Videos you encounter in real life are much lighter, because they aren’t stored in `float32`, and they’re typically compressed by a large factor (such as in the `MPEG` format)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47aade",
   "metadata": {},
   "source": [
    "### Broadcasting:\n",
    "> When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimension and works its way left. Two dimensions are compatible when\n",
    "\n",
    "> 1. They are equal, or\n",
    "> 2. One of them is 1.\n",
    "\n",
    "> If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes.\n",
    "\n",
    "> Input arrays do not need to have the same number of dimensions. The resulting array will have the same number of dimensions as the input array with the greatest number of dimensions, where the size of each dimension is the largest size of the corresponding dimension among the input arrays. Note that missing dimensions are assumed to have size one.\n",
    "\n",
    "> For example, if you have a 256x256x3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:\n",
    "\n",
    ">>* `Image  (3d array): 256 x 256 x 3`\n",
    ">>* `Scale  (1d array):             3`\n",
    ">>* `Result (3d array): 256 x 256 x 3`\n",
    ">* When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or “copied” to match the other.\n",
    "\n",
    ">In terms of implementation, **no new rank-3 tensor is created for scaling values**, because that would be terribly inefficient. The **broadcasting operation is entirely virtual**: it happens **at the algorithmic level rather than at the memory level**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330a4434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3,), (1, 3), array([[11., 22., 33.]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([10.0, 20.0, 30.0])\n",
    "b = np.array([[1.0, 2.0, 3.0]])\n",
    "a.shape, b.shape, a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e72dd9d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8, 12],\n",
       "        [ 8, 12]]),\n",
       " array([[ 6,  6],\n",
       "        [14, 14]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication is not cummtative.\n",
    "\n",
    "c= np.array([[2,2],[2,2]])\n",
    "d = np.array([[1,2],[3,4]])\n",
    "\n",
    "np.dot(c,d), np.dot(d,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbbbdd1",
   "metadata": {},
   "source": [
    ">* More generally, you can take the dot product between higher-dimensional tensors, following the same rules for shape compatibility as outlined earlier for the 2D case:\n",
    ">>* `(a, b, c, d) • (d,)   → (a, b, c)`\n",
    ">>* `(a, b, c, d) • (d, e) → (a, b, c, e)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645bfd73",
   "metadata": {},
   "source": [
    "##  Geometric interpretation of tensor operations:\n",
    ">`(idea is fully discussed in this chapter)`\n",
    "\n",
    ">* Neural network basic operations are `(W.X +b)` which is a linear transformation and rotation of the space.\n",
    ">* As we optimize the neural network, we try to find the best space that separates our categories if we are in classification task for example.\n",
    ">* Think of it as this crampled paper example: before the paper is crampled, it was a flat sheet.\n",
    ">* In neural network, our data got to us as crampled paper and we try to do the best geometrical tranformations (via training) that help us find this flat sheet.\n",
    "### Some Geometrical transformation:\n",
    "> ![1](https://github.com/OmarAllam22/Images-for-notebooks/blob/main/11.PNG?raw=true)\n",
    "> ![2](https://github.com/OmarAllam22/Images-for-notebooks/blob/main/22.PNG?raw=true)\n",
    "> ![3](https://github.com/OmarAllam22/Images-for-notebooks/blob/main/33.PNG?raw=true)\n",
    "> ![4](https://github.com/OmarAllam22/Images-for-notebooks/blob/main/44.PNG?raw=true)\n",
    "> ![5](https://github.com/OmarAllam22/Images-for-notebooks/blob/main/55.PNG?raw=true)\n",
    "> ![6](https://github.com/OmarAllam22/Images-for-notebooks/blob/main/66.PNG?raw=true)\n",
    "> ![7](https://github.com/OmarAllam22/Images-for-notebooks/blob/main/77.PNG?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67a75c",
   "metadata": {},
   "source": [
    "#### Optimizers:\n",
    ">* Optimizers aren't the same thing as gradient descent. But they are algorithms that defines the way in which gradient descent updates parameters. Therefore, there is ordinary gradient descent (either mini-batch, batch or stocastic) and there are SGD with momentum, RMSprop, and Adagrad. \n",
    "\n",
    ">* There is, for instance, `SGD with momentum`, as well as `Adagrad`, `RMSprop`, and several others. Such variants are known as **optimization methods or optimizers**. In particular, the concept of momentum, which is used in many of these variants, deserves your attention. \n",
    ">* `Momentum` addresses two issues with SGD: **convergence speed** and **local minima**. Momentum can **help in reachig gloabl minima** and not stucking at local one.\n",
    ">* Also `Learning Schedule`, method used to adjust learning rate over time, has effect on **reachig gloabl minima** and not stucking at local one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eaa189f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(0.)                      \n",
    "with tf.GradientTape() as tape:          \n",
    "    y = 2 * x + 3                        \n",
    "grad_of_y_wrt_x = tape.gradient(y, x)  \n",
    "grad_of_y_wrt_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2daf974",
   "metadata": {},
   "source": [
    ">* In the above cell, there are two important notes:\n",
    "\n",
    ">>* `tf.Variable()` is a special type of tensor that is mutable, meaning its value can be changed during the execution of a **computation graph**. Unlike ordinary tensors, `tf.Variable` is designed to be **mutable** in a specific way that enables it to participate in computations within a TensorFlow graph. When you create a `tf.Variable`, TensorFlow tracks it as a node in the computational graph and automatically computes gradients for it during backpropagation. This means that when you update the value of a `tf.Variable`, TensorFlow knows that the graph has changed and can update any downstream operations that depend on that variable.\n",
    ">>>* This is because `Tensorflow` depends on the idea of **flow** of **tensors** through the computation graphs.\n",
    "\n",
    ">>* `GradientTape()`:  reecords operations for automatic differentiation.\n",
    "\n",
    ">>* Don't forget to intialize the weights `W` and `b` randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57ee222b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(784, 10), dtype=float32, numpy=\n",
       " array([[-0.25137883, -2.6181302 , -1.9342039 , ...,  1.3155369 ,\n",
       "         -0.2628116 ,  0.43288547],\n",
       "        [ 0.1335316 , -1.8421947 ,  0.10670638, ...,  1.3369033 ,\n",
       "          0.63244146,  0.40717515],\n",
       "        [-0.41456538,  0.42275426,  1.0531392 , ...,  0.4635477 ,\n",
       "          0.23076028, -0.4242389 ],\n",
       "        ...,\n",
       "        [ 0.01488625,  2.805472  ,  2.9826677 , ...,  0.8803037 ,\n",
       "          0.32808232,  0.10142536],\n",
       "        [ 0.05976688,  1.4644451 ,  3.1204963 , ...,  0.6548274 ,\n",
       "         -0.5716774 ,  0.32218298],\n",
       "        [ 0.22088124, -1.1825455 , -0.05075556, ..., -1.0533674 ,\n",
       "          0.5194074 ,  0.0714402 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       " array([  2.322889 , -20.620571 , -20.07185  ,   4.1506615, -11.206947 ,\n",
       "        -23.711704 , -10.409934 , -16.378399 ,   6.685594 ,  -4.8212214],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[ -5.473991 ],\n",
       "        [-32.67148  ],\n",
       "        [-33.582706 ],\n",
       "        [ -3.0674965],\n",
       "        [-26.597212 ],\n",
       "        [-33.643974 ],\n",
       "        [-22.288982 ],\n",
       "        [-31.827982 ],\n",
       "        [ -3.1471593],\n",
       "        [-17.707148 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([-156.91193], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another example:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# define the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(784,), activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# define the loss function\n",
    "def loss_fn(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# define the data\n",
    "x_train = tf.random.normal((1000, 784))\n",
    "y_train = tf.random.uniform((1000, 1), minval=0, maxval=2, dtype=tf.int32)\n",
    "\n",
    "# define the training loop\n",
    "for i in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x_train)\n",
    "        loss = loss_fn(y_train, y_pred)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2312ce",
   "metadata": {},
   "source": [
    "_______________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
